{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import sys\n",
    "import json_lines\n",
    "import pandas as pd\n",
    "import ijson\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from itertools import count\n",
    "from numpy import linspace, loadtxt, ones, convolve\n",
    "from random import randint\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# head -n 1 2017-07-05.jsonl\n",
    "# wc -l 2017-07-05.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will obtain the root key values of the data we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_id', 'tag', 'btag', 'events', 'version', 'entities', 'story_id', 'blog_rank', 'companies', 'article_id', 'ca_webrank', 'crawled_at', 'lexalytics', 'source_url', 'article_url', 'author_name', 'ca_newsrank', 'ca_printcir', 'provider_id', 'article_type', 'author_email', 'event_groups', 'harvested_at', 'published_at', 'story_volume', 'accern_alerts', 'alexa_traffic', 'article_image', 'article_title', 'ca_rechpermil', 'event_summary', 'first_mention', 'article_source', 'article_content', 'article_summary', 'source_category', 'story_sentiment', 'accern_sentiment', 'story_saturation', 'article_sentiment', 'event_author_rank', 'event_source_rank', 'event_impact_score', 'overall_author_rank', 'overall_source_rank', 'story_alexa_traffic']\n"
     ]
    }
   ],
   "source": [
    "root_keys = []\n",
    "with open('2017-07-05.jsonl', 'rb') as f:\n",
    "    for item in json_lines.reader(f):\n",
    "        for key in item.keys():\n",
    "            root_keys.append(key)\n",
    "        break\n",
    "print(root_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will read the relevant data into a pandas dataframe. Due to the large size of the dataset (~20 GB), we will leave out data (keys) which won't be used immediately for anomaly detection purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "keys_to_remove = ['_id', 'btag', 'version', 'article_id', 'lexalytics', 'source_url', 'article_url', 'ca_newsrank', 'provider_id', 'author_email', 'harvested_at', 'published_at', 'accern_alerts', 'article_image', 'article_title', 'ca_rechpermil', 'event_summary', 'article_source', 'article_content', 'article_summary']\n",
    "item_list = []\n",
    "# count = 0\n",
    "with open('2017-07-05.jsonl', 'rb') as f:\n",
    "    for item in json_lines.reader(f):\n",
    "#         count += 1\n",
    "        for key in keys_to_remove:\n",
    "            item.pop(key)\n",
    "        item_list.append(item)\n",
    "#         if count > 500000:\n",
    "#             break\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # keys_to_remove = ['_id', 'btag', 'version', 'article_id', 'lexalytics', 'source_url', 'article_url', 'ca_newsrank', 'provider_id', 'author_email', 'harvested_at', 'published_at', 'accern_alerts', 'article_image', 'article_title', 'ca_rechpermil', 'event_summary', 'article_source', 'article_content', 'article_summary']\n",
    "# # item_list = []\n",
    "\n",
    "# break_count = [10]\n",
    "\n",
    "# def hollow_func(break_count):\n",
    "#     count = 0\n",
    "#     with open('2017-07-05.jsonl', 'rb') as f:\n",
    "#         for item in json_lines.reader(f):\n",
    "#             count += 1\n",
    "#             if count > break_count:\n",
    "#                 break\n",
    "#     return count\n",
    "\n",
    "# counts = Parallel(n_jobs=6)(delayed(hollow_func) for i in break_count)\n",
    "\n",
    "# print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract columns\n",
    "id_list = []\n",
    "tags = []\n",
    "events = []\n",
    "story_ids = []\n",
    "blog_ranks = []\n",
    "companies = []\n",
    "ca_webranks = []\n",
    "crawled_at = []\n",
    "ca_newsranks = []\n",
    "article_types = []\n",
    "event_groups = []\n",
    "story_volumes = []\n",
    "alexa_traffic = []\n",
    "first_mentions = []\n",
    "source_categories = []\n",
    "story_sentiments = []\n",
    "accern_sentiments = []\n",
    "story_saturations = []\n",
    "article_sentiments = []\n",
    "event_author_ranks = []\n",
    "event_source_ranks = []\n",
    "event_impact_scores = []\n",
    "overall_author_ranks = []\n",
    "overall_source_ranks = []\n",
    "story_alexa_traffics = []\n",
    "\n",
    "count = 0\n",
    "with open('2017-07-05.jsonl', 'rb') as f:\n",
    "    for item in json_lines.reader(f):\n",
    "        id_list.append(item['_id'])\n",
    "        tags.append(item['tag'])\n",
    "        events.append(item['events'])\n",
    "        story_ids.append(item['story_id'])\n",
    "        blog_ranks.append(item['blog_rank'])\n",
    "        companies.append(item['companies'])\n",
    "        ca_webranks.append(item['ca_webrank'])\n",
    "        crawled_at.append(item['crawled_at'])\n",
    "        ca_newsranks.append(item['ca_newsrank'])\n",
    "        article_types.append(item['article_type'])\n",
    "        event_groups.append(item['event_groups'])\n",
    "        story_volumes.append(item['story_volume'])\n",
    "        alexa_traffic.append(item['alexa_traffic'])\n",
    "        first_mentions.append(item['first_mention'])\n",
    "        source_categories.append(item['source_category'])\n",
    "        story_sentiments.append(item['story_sentiment'])\n",
    "        accern_sentiments.append(item['accern_sentiment'])\n",
    "        story_saturations.append(item['story_saturation'])\n",
    "        article_sentiments.append(item['article_sentiment'])\n",
    "        event_author_ranks.append(item['event_author_rank'])\n",
    "        event_source_ranks.append(item['event_source_rank'])\n",
    "        event_impact_scores.append(item['event_impact_score'])\n",
    "        overall_author_ranks.append(item['overall_author_rank'])\n",
    "        overall_source_ranks.append(item['overall_source_rank'])\n",
    "        story_alexa_traffics.append(item['story_alexa_traffic'])\n",
    "        \n",
    "#         count += 1\n",
    "#         if count > 5:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(accern_sentiments[1:100])\n",
    "# plt.show()\n",
    "(sys.getsizeof(id_list) + sys.getsizeof(tags) + sys.getsizeof(events) + sys.getsizeof(story_ids) +\n",
    "sys.getsizeof(blog_ranks) +\n",
    "sys.getsizeof(companies) + \n",
    "sys.getsizeof(ca_webranks) +\n",
    "sys.getsizeof(crawled_at) +\n",
    "sys.getsizeof(ca_newsranks) +\n",
    "sys.getsizeof(article_types) +\n",
    "sys.getsizeof(event_groups) +\n",
    "sys.getsizeof(story_volumes) +\n",
    "sys.getsizeof(alexa_traffic) +\n",
    "sys.getsizeof(first_mentions) +\n",
    "sys.getsizeof(source_categories) +\n",
    "sys.getsizeof(story_sentiments) +\n",
    "sys.getsizeof(accern_sentiments) +\n",
    "sys.getsizeof(story_saturations) +\n",
    "sys.getsizeof(article_sentiments) +\n",
    "sys.getsizeof(event_author_ranks) +\n",
    "sys.getsizeof(event_source_ranks) +\n",
    "sys.getsizeof(event_impact_scores) +\n",
    "sys.getsizeof(overall_author_ranks) +\n",
    "sys.getsizeof(overall_source_ranks) +\n",
    "sys.getsizeof(story_alexa_traffics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moving_average(data ,window_size):\n",
    "    \"\"\" Computes moving average using discrete linear convolution of two one dimensional sequences.\n",
    "    Args:\n",
    "    -----\n",
    "            data (pandas.Series): independent variable\n",
    "            window_size (int): rolling window size\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "            ndarray of linear convolution\n",
    "\n",
    "    References:\n",
    "    ------------\n",
    "    [1] Wikipedia, \"Convolution\", http://en.wikipedia.org/wiki/Convolution.\n",
    "    [2] API Reference: https://docs.scipy.org/doc/numpy/reference/generated/numpy.convolve.html\n",
    "\n",
    "    \"\"\"\n",
    "    window = np.ones(int(window_size))/float(window_size)\n",
    "    return np.convolve(data, window, 'same')\n",
    "\n",
    "def explain_anomalies_rolling_std(y, window_size, sigma=1.0):\n",
    "    \"\"\" Helps in exploring the anamolies using rolling standard deviation\n",
    "    Args:\n",
    "    -----\n",
    "        y (pandas.Series): independent variable\n",
    "        window_size (int): rolling window size\n",
    "        sigma (int): value for standard deviation\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        a dict (dict of 'standard_deviation': int, 'anomalies_dict': (index: value))\n",
    "        containing information about the points indentified as anomalies\n",
    "    \"\"\"\n",
    "    avg = moving_average(y, window_size)\n",
    "    avg_list = avg.tolist()\n",
    "    residual = y - avg\n",
    "    # Calculate the variation in the distribution of the residual\n",
    "    testing_std = pd.rolling_std(residual, window_size)\n",
    "    testing_std_as_df = pd.DataFrame(testing_std)\n",
    "    rolling_std = testing_std_as_df.replace(np.nan,\n",
    "                           testing_std_as_df.ix[window_size - 1]).round(3).iloc[:,0].tolist()\n",
    "    std = np.std(residual)\n",
    "    return {'stationary standard deviation': round(std, 3),\n",
    "           'anomalies_dict': collections.OrderedDict([(index, y_i)\n",
    "                                                     for index, y_i, avg_i, rs_i in zip(count(),\n",
    "                                                                                       y, avg_list, rolling_std)\n",
    "                                 if (y_i > avg_i + (sigma * rs_i)) | (y_i < avg_i - (sigma *rs_i))])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = explain_anomalies_rolling_std(sigma=3.0, window_size=10000, y=accern_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomalies['stationary standard deviation']\n",
    "# anomalies['anomalies_dict']\n",
    "# print(accern_sentiments[4414458])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
